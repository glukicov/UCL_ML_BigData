{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    " # Lecture 1: Introduction to machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2020-01-16 16:35:52\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Version: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Description and objectives\n",
    "\n",
    "This module covers how to apply machine learning techniques to large data-sets -- so-called *big-data*. \n",
    "\n",
    "An introduction to machine learning (ML) is presented to provide a general understanding of the concepts of machine learning, common machine learning techniques, and how to apply these methods to data-sets of moderate sizes.  \n",
    "\n",
    "Deep learning and computing frameworks to scale machine learning techniques to big-data are then presented. \n",
    "\n",
    "Scientific data formats and data curation methods are also discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Syllabus\n",
    "\n",
    "Foundations of ML (e.g. overview of ML, training, data wrangling, scikit-learn, performance analysis, gradient descent), data formats and curation (e.g. serialisation, deserialisation, domain specific languages, markup languages, controlled vocabularies, semantic models), ML methods (e.g. logistic regression, SVMs, ANNs, decision trees, ensemble learning and random forests, dimensionality reduction), deep learning and scaling to big-data (e.g. TensorFlow, Deep ANNs, CNNs, RNNs, Autoencoders, Cloud computing) and applications of ML in astrophysics, high-energy physics and industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Students should have a reasonable working knowledge of Python, some familiarity with working in the command line environment in Linux/Unix based operating systems, and a general understanding of elementary mathematics, including linear algebra and calculus. \n",
    "\n",
    "No previous familiarity with machine learning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "\n",
    "### Textbooks \n",
    "\n",
    "- VanderPlas, \"*Python data science handbook*\", O'Reilly, 2017, ISBN 9781491912058\n",
    "  ([Example code](https://github.com/jakevdp/PythonDataScienceHandbook))\n",
    "\n",
    "- Geron, \"*Hands-on machine learning with Scikit-Learn and TensorFlow*\", O'Reilly, 2017, ISBN 9781491962299\n",
    "  ([Example code](https://github.com/ageron/handson-ml))\n",
    "\n",
    "- Goodfellow, Bengio, Courville (GBC), \"*Deep learning*\", MIT Press, 2016, ISBN 9780262035613\n",
    "  ([Website](http://www.deeplearningbook.org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code frameworks and libraries\n",
    "\n",
    "- [Scikit-Learn](http://scikit-learn.org/stable/)\n",
    " \n",
    "- [TensorFlow](https://www.tensorflow.org/)\n",
    " \n",
    " \n",
    "### Tutorials \n",
    " \n",
    "- [Scikit-Learn tutorial](https://github.com/jakevdp/sklearn_tutorial), VanderPlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Jupyter notebooks\n",
    "\n",
    "Each lecture has an accompaning Jupyter notebook, with executable code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These slides are a Jupyter notebook.\n",
    "\n",
    "Notebooks can be viewed in slide mode using [RISE](https://github.com/damianavila/RISE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The supporting Jupyter notebooks thus serve as the course *slides*, *lecture notes*, and *examples*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Course philosophy\n",
    "\n",
    "This is a practical, hands-on course.  While we will cover basic concepts and background theory (but not in great mathematical depth or rigor), a large component of the course will focus on implementing and running machine learning algorithms.\n",
    "\n",
    "The lectures will include many code examples and exercises to be completed in class.\n",
    "\n",
    "Versions of the course Jupyter notebooks -- without solutions to exercises -- will be made available before each lecture.  Students can then follow examples by running code live in class (and inspecting variables and making modifications).  Exercises will be completed by adding to these notebooks during class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Schedule\n",
    "\n",
    "We will have one 3 hour session of three back-to-back lectures each week, with a short break between lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Assessment\n",
    "\n",
    "\n",
    "- Coursework: 30%\n",
    "- Exam: 70%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coursework\n",
    "\n",
    "Coursework will involve downloading a Jupyter notebook, which you will need to complete.  \n",
    "\n",
    "Throughout the notebook you will need to complete code, analytic exercises and descriptive answers. Much of the grading of the coursework will be performed automatically.\n",
    "\n",
    "The coursework will be issued after the first 9 lectures, which contain the material required to complete the coursework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exam\n",
    "\n",
    "*Answer FOUR questions* of the FIVE questions provided.\n",
    "\n",
    "Exam duration is *TWO hours*.\n",
    "\n",
    "Each question has equal mark (20 marks per question).\n",
    "\n",
    "Markers place importance on clarity and a portion of the marks are awarded for clear descriptions, answers, drawings, and diagrams, and attention to precision in quantitative answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing setup\n",
    "\n",
    "Students are expected to bring their own laptops to class in order to run notebooks and complete examples.\n",
    " \n",
    "All examples are implemented in Python 3. \n",
    "\n",
    "The main Python libraries that are required include the following:\n",
    "- `numpy` \n",
    "- `scipy` \n",
    "- `matplotlib` \n",
    "- `scikit-learn` \n",
    "- `ipython`/`jupyter` \n",
    "- `seaborn`\n",
    "- `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A python environment is required to run the notebooks.  \n",
    "\n",
    "Instructions for using Anaconda are included here, although other Python setups could be used.  An environment to run the notebooks can be set up with the versions of the libraries in `requirements.txt` (included below), following the steps below in terminal (MacOS) or anaconda prompt (Windows): \n",
    "\n",
    "1. Create an environment named MLBDenv with the following libraries in the `requirements.txt` file installed. (conda-forge is needed for the rdflib library) \n",
    "`conda config --append channels conda-forge conda create --name MLBDenv python=3.7 --file requirements.txt`  \n",
    "\n",
    "2. Go into the `MLBDenv` environment and then create a kernel which can run the notebook in this environment.\n",
    "`conda activate MLBDenv python -m ipykernel install --user --name MLBDenv --display-name \"MLBDenv\" conda deactivate` \n",
    "\n",
    "3. When running a jupyter notebook, we can switch to the MLBDenv environment by clicking on the tab Kernel -> Change kernel -> MLBDenv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Content of requirements.txt:\n",
    "\n",
    "```matplotlib=3.1.1\n",
    "scipy=1.3.1\n",
    "pandas=0.25.3 \n",
    "scikit-learn=0.21.3\n",
    "seaborn=0.9.0 \n",
    "pillow=6.2.1 \n",
    "ipykernel=5.1.3 \n",
    "tensorflow=1.14.0\n",
    "mako=1.1.0 \n",
    "sqlalchemy=1.3.12 \n",
    "pyyaml=5.2\n",
    "ply=3.11\n",
    "rdflib=4.2.2\n",
    "lxml=4.4.2 ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artifical intelligence (AI)\n",
    "\n",
    "Ironically...\n",
    "\n",
    "- Solving \"computational problems\" that are difficult for humans is straightforward for machines (i.e. problems described by list of formal mathematical rules).\n",
    "\n",
    "- Solving \"intuitive problems\" that are easy for humans is difficult for machines (i.e. problems difficult to describe formally).\n",
    "\n",
    "This is often known as [Moravec's paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox) (although formal definition is a little more specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution is to allow computers to learn from experience and to build an understanding of the world through a hierarchy of concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Knowledge base approach\n",
    "\n",
    "Hard-code knowledge about world in formal set of rules and use logical inference.\n",
    "\n",
    "Very difficult to capture complexity of intuitive problems in this manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine learning (ML)\n",
    "\n",
    "Arthur Samuel (1959):\n",
    "> \"[Machine learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\"\n",
    "\n",
    "<br>\n",
    "\n",
    "Tom Mitchell (1997):\n",
    "> \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uses of machine learning\n",
    "\n",
    "1. **Prediction:** Predict outcome given data.\n",
    "2. **Inference:** Better understand data (and their distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data representations\n",
    "\n",
    "\n",
    "Performance of machine learning depends on representation of data given.\n",
    "\n",
    "Data represented to learning algorithm as \"*features*\".\n",
    "\n",
    "Traditional approach to machine learning involved *\"feature engineering\"*, where a practitioner with domain expertise would develop techniques to extract informative features from raw data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of features\n",
    "\n",
    "- Computer visions: edges and corners\n",
    "- Spam: frequency of words\n",
    "- Character recognition: histograms of black pixels along rows/columns, number of holes, number of strokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning representations\n",
    "\n",
    "Alternative is to learn features.\n",
    "\n",
    "- Can discover informative features from data.\n",
    "- Minimal human intervention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Approaches to representation learning\n",
    "\n",
    "- Dedicated feature learning, e.g. autoencoder combining encoder and decoder.\n",
    "- Representation learning integral to overall machine learning technique, e.g. deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Approaches to artifical intelligence\n",
    "\n",
    "<center><img src=\"Lecture01_Images/ai_venn_diagram.png\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "[Image credit: [GBC](http://www.deeplearningbook.org/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AI pipelines\n",
    "\n",
    "<center><img src=\"Lecture01_Images/ai_approaches.png\" style=\"width: 350px;\"/></center>\n",
    "\n",
    "[Image credit: [GBC](http://www.deeplearningbook.org/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The unreasonable effectiveness of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As society becomes increasing digitised, the volume of available data is exploding. \n",
    "\n",
    "A significant increase in the volume of data can lead to dramatic increases in the performance of machine learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Term coined in Halevy, Norbig & Pereira, 2009, [*The unreasonable effectiveness of data*](http://goo.gl/q6LaZ8).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Size of benchmark data-sets\n",
    "\n",
    "<center><img src=\"Lecture01_Images/data_sizes.png\" style=\"width: 1200px;\"/></center>\n",
    "\n",
    "[Image credit: [GBC](http://www.deeplearningbook.org/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Size of data can have a larger impact than algorihm\n",
    "\n",
    "<center><img src=\"Lecture01_Images/importance_of_data.png\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "Source: Banko & Brill, 2001, [*Scaling to very very large corpora for natural language disambiguation*](http://goo.gl/R5enIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "> As a rule of thumb, a supervised deep learning algorithm will perform reasonably well with around 5,000 labelled samples.  \n",
    "\n",
    "> With 10 million samples, it will match or exceed human performance. \n",
    "\n",
    "[Source: [GBC](http://www.deeplearningbook.org/)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, in many cases very large datasets are not available and in some cases not possible.  \n",
    "\n",
    "Hence, developing effective algorithms remains critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classes of machine learning\n",
    "\n",
    "1. **Supervised:** Learn to predict output given input (given labelled training data).\n",
    "2. **Unsupervised:** Discover internal representation of input.\n",
    "3. **Reinforcement:** Learn action to maximise payoff.\n",
    "\n",
    "<center><img src=\"Lecture01_Images/supervised_unsupervised_learning.png\" style=\"width: 1200px;\"/></center>\n",
    "\n",
    "[[Image source](http://beta.cambridgespark.com/courses/jpm/01-module.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised learning\n",
    "\n",
    "Learn to predict output given input (given labelled training data).\n",
    "\n",
    "1. **Regression:** Target output is a (real) number, <br>\n",
    "    e.g. estimate flux intensity.\n",
    "\n",
    "2. **Classification:** Target output is a class label,<br>\n",
    "    e.g. classify galaxy morphology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How supervised learning works\n",
    "\n",
    "- Select model defined by function $f$, and model target $y$ from inputs $x$ by\n",
    "$$y = f(x, \\theta),$$\n",
    "where $\\theta$ are the parameters of the model that are learnt during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Learning typically involves minimising the difference between the inputs and outputs for the model, given a training data-set (more on training, validation and test data-sets later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "Discover internal representation of input.\n",
    "\n",
    "1. **Cluster finding:** Learn cluster of similar structure in data.\n",
    "2. **Density estimation:** Learn representations of data (probability distributions).\n",
    "3. **Dimensionality reduction:** Provides compact, low-dimensional representation of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Unsupervised learning examples\n",
    "\n",
    "Anomaly detection, clustering groups of similar objects, visualising high-dimensional data in 2D or 3D plots are  examples of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "Learn action to maximise payoff.\n",
    "\n",
    "- Output is an action or sequence of actions and the only supervisory signal is an occasional numerical (scalar) reward.\n",
    "- Difficult since rewards are delayed.\n",
    "- Not covered in this course.\n",
    "\n",
    "<center><img src=\"Lecture01_Images/rl_interaction.png\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "[[Image credit](https://www.analyticsvidhya.com/blog/2016/12/getting-ready-for-ai-based-gaming-agents-overview-of-open-source-reinforcement-learning-platforms/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reinforcement learning examples\n",
    "\n",
    "Go, playing computer games, driverless cars, self navigating vaccum cleaners, scheduling of elevators are all applications of reinforcement learning.\n",
    "\n",
    "E.g. [Google [DeepMind] machine learns to master video games](http://www.bbc.co.uk/news/science-environment-31623427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "Machine *learning* often involves solving an *optimization* problem, i.e. finding the parameters $\\theta$ of the model $f$ to best represent the training data (for supervised learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Objective function\n",
    "\n",
    "Typically maximise/minimise some goodness-of-fit/cost function.\n",
    "\n",
    "\n",
    "### Example of convex objective function\n",
    "\n",
    "<center><img src=\"Lecture01_Images/optimization_convex.png\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "[Image credit: Kirkby, UC Irvine, LSST Dark Energy Summer School 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of non-convex objective function\n",
    "\n",
    "<center><img src=\"Lecture01_Images/optimization_nonconvex.jpg\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "[[Image source](https://cs.hse.ru/data/2016/08/26/1121363361/moml.jpg)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using gradients to optimize objective function (i.e. perform training)\n",
    "\n",
    "- **(Batch) Gradient descent:** Use all data at each iteration (full dimension).\n",
    "- **Stochastic gradient descent:** Use a random data-point at each iteration (1 dimension).\n",
    "- **Backpropagation:** propagate errors backwards through networks.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Lecture01_Images/optimization_gd.png\" style=\"width: 400px;\"/></td>\n",
    "    <td><img  src=\"Lecture01_Images/optimization_sgd.png\" style=\"width: 400px;\"/></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center>Batch gradient descent</center></td>\n",
    "    <td><center>Stochastic gradient descent</center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "[[Image source](http://www.holehouse.org/mlclass)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch and online learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch learning\n",
    "\n",
    "Algorithm is trained using all available training data at once.\n",
    "\n",
    "Also called *offline learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Requires substantial resources (CPU, memory space, disk space).\n",
    "- If want to add new training data, must re-train from scratch on new full set of data (i.e. not just the new data but also the old data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Online learning\n",
    "\n",
    "Algorithm is trained using a sub-set of the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each learning step does *not* require substantial resources. \n",
    "- Can integate new training data on the fly.\n",
    "- May be able to throw away data once used it (although might not want to).\n",
    "- If fed bad data, performance will decline.\n",
    "- Noisy training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting and underfitting\n",
    "\n",
    "- **Problem:** The learned model may fit the training set extremely well but fail to generalise to new examples.\n",
    "\n",
    "### 1D example\n",
    "\n",
    "<center><img src=\"Lecture01_Images/overfitting_1d.png\" style=\"width: 1200px;\"/></center>\n",
    "\n",
    "[[Image source](http://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2D example\n",
    "\n",
    "<center><img src=\"Lecture01_Images/overfitting_2d.png\" style=\"width: 1200px;\"/></center>\n",
    "\n",
    "[[Image source](https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/assets/dpln_0107.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Techniques to avoid overfitting\n",
    "\n",
    "- Reduce complexity of model.\n",
    "- Regularization:\n",
    "    - Place additional constraints (priors) on features/parameters.\n",
    "    - E.g. smoothness of parameters, sparsity of model (i.e. limit complexity).\n",
    "- Split data into training, validation and test sets (e.g. cross-validation). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## No free lunch theorem\n",
    "\n",
    "Essentially, all algorithms are equivalent when performance is averaged over all possible problems.\n",
    "\n",
    "Consequently, there is no a priori model that is guaranteed to work best on all problems.\n",
    "\n",
    "(Wolpert, 1996, [*The lack of a priori distinctions between learning algorithms*](http://goo.gl/q6LaZ8))\n",
    "\n",
    "It is therefore a matter of validating models empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and test datasets\n",
    "\n",
    "Split data into training and test sets (e.g. 80% for training and 20% for testing).\n",
    "\n",
    "The model is trained on the *training set* and then tested on the *test set*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**No data used in training the method is then used to evaluate it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Error rate on the test set is called the *generalization error* or *out of sample error*.\n",
    "\n",
    "If the training error is low but the generalization error is high, it suggests the model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Many machine learning algorithms contain hyperparameters to control the model.  \n",
    "\n",
    "One (**bad**) approach is to evaluate alternative models defined by different hyperparameters on test set and select the model that performs best.\n",
    "\n",
    "However, this optimizes the model for the test set and may not generalise to other data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "\n",
    "A better approach is to split the data into three sets: \n",
    "1. Training set\n",
    "2. Validation set\n",
    "3. Test set\n",
    "\n",
    "Train models on the training set and evaluate different models (with different hyperparameters) on the validation set.\n",
    "\n",
    "Only once the final model to be used is fully specified should it be applied to the test set to estimate its generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "A disadvantage of the previous approach is that less data are available for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Cross-validation* addresses this issue by performing a sequence of fits where each subset of the data is used both as a training set and a validation set.\n",
    "\n",
    "\n",
    "<center><img src=\"Lecture01_Images/2-fold-CV.png\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "\n",
    "[Image credit: [VanderPlas](https://github.com/jakevdp/PythonDataScienceHandbook)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get validation accuracy scores for each trial, which could be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extension to n-fold cross-validation\n",
    "\n",
    "<center><img src=\"Lecture01_Images/5-fold-CV.png\" style=\"width: 700px;\"/></center>\n",
    "\n",
    "[Image credit: [VanderPlas](https://github.com/jakevdp/PythonDataScienceHandbook)]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
