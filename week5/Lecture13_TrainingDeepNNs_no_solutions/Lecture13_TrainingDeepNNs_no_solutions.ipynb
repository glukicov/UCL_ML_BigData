{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Lecture 13: Training deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2020-02-12 22:23:22\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Version: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training typically relies on gradients.\n",
    "\n",
    "*Vanishing gradients problem*: For deep networks, gradients in lower layers can become very small.  Hence, corresponding weights are not updated during training.\n",
    "\n",
    "*Exploding gradients problem*: In some situations (typically recurrent neural networks) gradients can become very large.  Hence, weight updates are very large and the training algorithm may not converge.\n",
    "\n",
    "In general deep neural networks can suffer from *unstable gradients*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problematic activation functions\n",
    "\n",
    "One common cause of vanishing gradients in the past was the use of the sigmoid activation function (and unit Gaussian initialisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -0.2, 1.2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a6b8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    " \n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance of outputs grows at each layer.  Final layers essentially saturate.  Gradients on final layers then very small and when propagate gradients back with back-propagation then get vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weight initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this problem need signals and gradents to *not* decay as propagating through network.\n",
    "\n",
    "Avoid decaying signals/gradients by promoting equal variance at outputs and inputs of layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can be promoted by random initialisation of weights to follow Gaussian with standard deviation:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\text{Sigmoid activation:} \\quad\\quad & \\sigma = \\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}} \\\\\n",
    "\\text{Hyperbolic tangent activation:} \\quad\\quad & \\sigma = 4\\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}} \\\\\n",
    "\\text{ReLU activation:} \\quad\\quad & \\sigma = \\sqrt{2}\\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "where $n_{\\rm inputs}$ and $n_{\\rm outputs}$ are the number of input and output nodes, respectively, for the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weight initialisation in Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcewen/anaconda3/envs/tensorflow_py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\") # He initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-saturating activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation behaves much better than the sigmoid in deep networks since it does not saturate for positive values (and it is fast to compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, the ReLU does suffer from the *dying neuron* problem.\n",
    "\n",
    "In this senario neurons effectively die and only output zero.  The neuron is unlikely to come back to life since the gradient of the ReLU activation function is zero for negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "The *leaky ReLU* avoids this problem and is defined by\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}_\\alpha(z) = \\max(\\alpha z, z),\n",
    "$$\n",
    "\n",
    "where the hyperparameter $\\alpha$ defines how much the leaky ReLU leaks (typically $\\alpha=0.01$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise: plot the Leaky ReLU activation function for $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ELU\n",
    "\n",
    "Another alternative is the *exponental linear unit* (ELU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise: plot the ELU activation function for $\\alpha=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Properties:\n",
    "- Non-zero gradient for $z<0$ to avoid dying neuron issue.\n",
    "- Smooth so gradients well defined.\n",
    "- But is slower to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ELU in Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While weight normalisation can reduce gradient problems at the beginning of training, it does not guarantee that these problems won't resurface during training.\n",
    "\n",
    "*Batch normalisation* adds normalisation during training to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consists of zero-centering and normalising inputs just before the activation function, followed by shifting and scaling the result.  The shift and scale are considered additional parameters that are learnt during training.\n",
    "\n",
    "This approach allows training to select the appropriate scale and shift (mean) for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The mean and standard deviation of the unnormalised inputs are computed for each mini-batch, hence the name *batch normalisation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the trained network is applied to the test set there are no batches, so instead the entire *training* set's mean and standard deviation are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch normalisation in Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, \n",
    "                                    training=training, momentum=0.9) \n",
    "    # Exponential decay is applied when computing running averages, hence the momentum parameter.\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Python `partial` function can be handy for defining a new function with some arguments set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "## Exercise: build a deep NN with ELU activation functions and batch normalisation and apply it to MINST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pretraining and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep network trained for one task can often be adapted for a similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reuse lower layers of network trained for another task.\n",
    "\n",
    "<center><img src=\"Lecture13_Images/transfer_learning.png\" style=\"height: 350px;\"/></center>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For transfer learning to be successful the data must have similar low-level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reusing a Tensor Flow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in previous lectures we can save and load computational graphs in addition to parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation graph is saved in `*.meta` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for op in tf.get_default_graph().get_operations():\n",
    "#    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "training = tf.get_default_graph().get_tensor_by_name(\"training:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"train/GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9504\n",
      "1 Test accuracy: 0.9543\n",
      "2 Test accuracy: 0.9565\n",
      "3 Test accuracy: 0.957\n",
      "4 Test accuracy: 0.9587\n",
      "5 Test accuracy: 0.9604\n",
      "6 Test accuracy: 0.9614\n",
      "7 Test accuracy: 0.9623\n",
      "8 Test accuracy: 0.9642\n",
      "9 Test accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can then also revise the computational graph...\n",
    "\n",
    "Typically will want to replace the upper layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Freezing lower layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower layers of the first network have already learnt low-level features for the first task, so they can be reused as they are. \n",
    "\n",
    "That is, freezing their weights so that they are not altered during subsequent training of the new network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can be acheived by providing restricted list of training variables to optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "#                                scope=\"hidden2|outputs\")\n",
    "# training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Caching the frozen layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights of the frozen layers don't change, their output for a given training instance also does not change.\n",
    "\n",
    "Caching these outputs can provide a considerable computational saving since the entire data-set needs to be processed multiple times.\n",
    "\n",
    "But can result in very large additional storage requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model zoos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many trained Tensor Flow models are available at \n",
    "[https://github.com/tensorflow/models](https://github.com/tensorflow/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improved optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although standard (stochastic) gradient descent is very effective it can still be slow for deep networks.\n",
    "\n",
    "There are a number of more advanced optimizers that provide improvements, e.g.:\n",
    "- Momentum optimization\n",
    "- Nesterov accelerated gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam optimization\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall gradient descent, with cost function $J(\\theta)$ and gradients $\\nabla_\\theta J(\\theta)$, proceeds simply by updating the weights $\\theta$ by taking a step $\\eta$ (learning rate) in the direction of the gradient:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum optimization uses the gradients to modify a momentum vector and uses the momentum to update the weights:\n",
    "\n",
    "1. $m \\leftarrow \\beta m + \\eta \\nabla_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gradient is used as an acceleration rather than speed.  Can help to traverse plateaus and to avoid local minima.\n",
    "\n",
    "The additional hyperparameter $\\beta$ is introduced as a friction term to avoid the momentum growing too large (typically $\\beta \\sim 0.9$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nesterov accelerated gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesterov accelerated gradient is a variant of momentum optimization where the gradient is computed further ahead in the direction of the momentum:\n",
    "\n",
    "1. $m \\leftarrow \\beta m + \\eta \\nabla_\\theta J(\\theta + \\beta m)$\n",
    "2. $\\theta \\leftarrow \\theta - m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general the momentum will be pointing toward the optimum and so Nesterov modification typically provides an improvement over standard momentum optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad scales down the gradient vector along the steepest direction by incorporating a gradient squared term:\n",
    "\n",
    "1. $s \\leftarrow s + \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta) \\oslash \\sqrt{s+\\epsilon}$\n",
    "\n",
    "Note that $\\otimes$ and $\\oslash$ are elementwise multiplication and division, respectively.\n",
    "\n",
    "The parameter $\\epsilon$ is introduced for numerical stability (typically $\\epsilon\\sim 10^{-10}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Basically, AdaGrad correspondings to an *adaptive learning rate* where the learning rate is decayed faster for steep directions.\n",
    "\n",
    "Consequently, it requires much less tuning of the learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"Lecture13_Images/ada_grad.png\" style=\"height: 500px;\"/></center>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp extends AdaGrad by introducing an exponential decay in the accumulated squared gradient:\n",
    "\n",
    "1. $s \\leftarrow \\beta s + (1-\\beta) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta) \\oslash \\sqrt{s+\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Typically $\\beta\\sim 0.9$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Avoids the problem where AdaGrad slows down too fast and so doesn't converge to the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimization combines momentum and RMSProp:\n",
    "\n",
    "1. $m \\leftarrow \\beta_1 m + (1-\\beta_1) \\nabla_\\theta J(\\theta)$\n",
    "2. $s \\leftarrow \\beta_2 s + (1-\\beta_2) \\nabla_\\theta J(\\theta)\\otimes\\nabla_\\theta J(\\theta)$\n",
    "3. $m \\leftarrow \\frac{m}{1-\\beta_1^{t}}$, where $t$ is the iteration number \n",
    "4. $s \\leftarrow \\frac{s}{1-\\beta_2^{t}}$, where $t$ is the iteration number\n",
    "5. $\\theta \\leftarrow \\theta - \\eta m \\oslash \\sqrt{s+\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Steps 3 and 4 are introduced to boost $m$ and $s$ at the beginnning of training (since they are initialed to 0 they can otherwise be low at the beginning).\n",
    "\n",
    "(Typically $\\beta_1 \\sim 0.9$, $\\beta_2 \\sim 0.999$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep networks have many parameters (sometimes millions) and so are prone to overfitting.\n",
    "\n",
    "Regularization therefore becomes increasingly important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple regularization strategy is to end training early, e.g. when performance on validation set starts to degrade.\n",
    "\n",
    "Although early stopping works well, other regularisation techniques can lead to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\ell_2$ and $\\ell_1$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tikhonov* regularization adopts $\\ell_2$ regularising term (also called *Ridge regression*):\n",
    "\n",
    "\n",
    "$$ R(\\theta) = \\frac{1}{2} \\sum_{j=1}^n \\theta_j^2 = \\frac{1}{2}  \\theta^{\\rm T}\\theta.$$\n",
    "\n",
    "\n",
    "*Lasso* regularization adopts $\\ell_1$ regularising term:\n",
    "\n",
    "$$ R(\\theta) =\\sum_{j=1}^n \\left\\vert \\theta_j \\right\\vert .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Elastic net* regularization provides a mix of Tikhonov and Lasso regularization, controlled by mix ratio $r$:\n",
    "\n",
    "$$ R(\\theta) =  r\\sum_{j=1}^n \\left\\vert \\theta_j \\right\\vert + \\frac{1-r}{2} \\sum_{j=1}^n \\theta_j^2.$$\n",
    "\n",
    "- For $r=0$, corresponds to Tikhonov regularization.\n",
    "- For $r=1$, corresponds to Lasso regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a very popular and effective regularlisation technique developed by [Geoff Hinton in 2012](http://www.jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dropout involves simply dropping each neuron for a given training set with probability $p$.\n",
    "\n",
    "<center><img src=\"Lecture13_Images/dropout.png\" style=\"height: 350px;\"/></center>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Dropout encourages each neuron to be as effective as possible individually and not to rely heavily on a few nearby neurons but to consider all input neurons carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The probability $p$ is called the *dropout rate* (typically $p \\sim 0.5$).\n",
    "\n",
    "After training the neurons don't get dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number of inputs of active neurons is lower when dropout is applied during training, than when the network is applied during testing.  \n",
    "\n",
    "For example, if $p=0.5$, on average there are half as many input neurons during training than when testing.  During testing each neuron will get an input signal (approximately) twice as large as during training.\n",
    "\n",
    "It is important to account for this difference.\n",
    "\n",
    "To compensate, after training each neurons input weights are multiplied by the keep probability $1-p$ before applying the network to test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation can be applied both as a regularization technique and to increase the volume of the training set.\n",
    "\n",
    "Essentially, new training instances are created from the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, for images, data augmentation can be performed by rotating, shifting, scaling, flipping, changing the contrast, ..., of the original images in the training data-set.\n",
    "\n",
    "<center><img src=\"Lecture13_Images/data_augmentation.png\" style=\"height: 500px;\"/></center>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Appropriate data augmentation strategies depend on the type of data under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically training instances are generated on the fly to avoid additional storage requirements.  \n",
    "\n",
    "Tensor Flow has built in functionality for many transformations for image data, making data augmentation for image data straightforward."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
