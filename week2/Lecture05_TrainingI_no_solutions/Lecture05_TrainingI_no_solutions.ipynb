{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Training I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2020-01-24 11:20:51\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Version: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear regression mode (scalar form)\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n$$\n",
    "\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- $n$ is the number of features.\n",
    "- $x_j$ is the $j$th feature, for $j=0,1,...,n$.\n",
    "- $\\theta_j$ is the $j$th model parameter\n",
    "<br>(with bias $\\theta_0$ and feature weights $\\theta_1, ..., \\theta_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear regression mode (vector form)\n",
    "\n",
    "$$\\hat{y} =\\theta^{\\rm T} x = h_\\theta(x)$$\n",
    "\n",
    "- $x$ is the instance feature vector $x=(x_0, x_1, ... x_n)^{\\rm T}$, where $x_0 = 1$.\n",
    "- $\\theta$ is the vector of model parameters $\\theta=(\\theta_0, \\theta_1, ... \\theta_n)^{\\rm T}$.\n",
    "- $\\cdot^{\\rm T}$ denotes transpose.\n",
    "- $h_\\theta(x)$ is the hypothesis function, with parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train linear regression\n",
    "\n",
    "Give training data $\\{ x^{(i)}, y^{(i)} \\}$, for $m$ instances $i=1,2,...,m$.\n",
    "\n",
    "Minimise mean square error (MSE):\n",
    "$$\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(\\theta^{\\rm T} x^{(i)} - y^{(i)}\\right)^2 .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concise matrix-vector notation\n",
    "\n",
    "Recall:\n",
    "- $x_j^{(i)}$ is (scalar) value of feature $j$ in $i$th training example.\n",
    "- $x^{(i)}$ is the (column) vector of features of the $i$th training example.\n",
    "- $y^{(i)}$ is the (scalar) value of target of the $i$th training example.\n",
    "- $n$ features and $m$ training instances\n",
    "\n",
    "Define:\n",
    "- Feature matrix: $X_{m \\times n} = [ x^{(1)},\\ x^{(2)},\\ ...,\\ x^{(m)}]^{\\rm T}$.\n",
    "- Target vector: $y_{m \\times 1} = [ y^{(1)},\\ y^{(2)},\\ ...,\\ y^{(m)}]^{\\rm T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recall features matrix and target vector\n",
    "\n",
    "<center><img src=\"Lecture05_Images/data-layout.png\" alt=\"data-layout\" style=\"height: 400px;\"/></center>\n",
    "\n",
    "[[Image source](https://github.com/jakevdp/sklearn_tutorial)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Minimising the MSE is equivalent to minimising the cost function\n",
    "\n",
    "$$C(\\theta) = \\frac{1}{m} (X \\theta - y)^{\\rm T}(X \\theta - y),$$\n",
    "\n",
    "where for notational convenience we denote the dependence on $\\theta$ only and consider $X$ and $y$ fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normal equations\n",
    "\n",
    "Minimise the cost function analytically \n",
    "\n",
    "$$\\min_\\theta\\ C(\\theta) = \\min_\\theta \\ (X \\theta - y)^{\\rm T}(X \\theta - y).$$\n",
    "\n",
    "Solution given by \n",
    "\n",
    "$$ \\hat{\\theta} = \\left( X^{\\rm T} X \\right)^{-1} X^{\\rm T} y. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of solving normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42) # To make this notebook's output stable across runs\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = 100 # number of training instances \n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise: Solve normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise: Make predictions using the fitted model for $x_1 = 0$ and $x_1 = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solve using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21509616]), array([[2.77011339]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the bias term\n",
    "X_b = np.c_[np.ones((m,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see image 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c435ce5eae61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlin_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_new' is not defined"
     ]
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computational complexity\n",
    "\n",
    "Solving the normal equation requires inverting $X^{\\rm T} X$, which is an $n \\times n$ matrix (where $n$ is the number of features).\n",
    "\n",
    "Inverting an $n \\times n$ matrix is naively of complexity $\\mathcal{O}(n^3)$ ($\\mathcal{O}(n^{2.4})$ if certain fast algorithms are used).\n",
    "\n",
    "Also, typically requires all training instances to be held in memory at once.\n",
    "\n",
    "Other methods are required to handle large data-sets, i.e. when considering large numbers of features and large numbers of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Lecture05_Images/optimization_gd.png\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "[[Image source](http://www.holehouse.org/mlclass)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient of the cost function is given by\n",
    "$$ \\frac{\\partial}{\\partial \\theta} C(\\theta)\n",
    "=\\frac{2}{m} X^{\\rm T}  \\left( X \\theta - y \\right) \n",
    "= \\left[\\frac{\\partial C}{\\partial \\theta_0},\\ \\frac{\\partial C}{\\partial \\theta_1},\\ ...,\\ \\frac{\\partial C}{\\partial \\theta_n} \\right]^{\\rm T} \n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Batch gradient descent algorithm defined by taking a step $\\alpha$ in the direction of the gradient:\n",
    "\n",
    "$$\\theta^{(t)} = \\theta^{(t-1)} - \\alpha  \\frac{\\partial C}{\\partial \\theta},$$\n",
    "\n",
    "where $t$ denotes the iteration number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This algorithm is called *batch* gradient descent since it uses the full *batch* of training data ($X$) at each iteration.  We will see other forms of gradient descent soon..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise: Implement gradient descent\n",
    "\n",
    "Implement gradient descent to estimate the parameters of the linear regression model considered before.\n",
    "\n",
    "Consider 1000 steps and $\\alpha=0.1$, starting from $\\theta^{(0)}=[1, 1]^{\\rm T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution found is very similar to that found by solving the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'X_b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-943491d3b48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtheta_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'X_b'"
     ]
    }
   ],
   "source": [
    "n_steps = 1000\n",
    "a = 0.1\n",
    "theta_0 = np.array([1,1]).T\n",
    "\n",
    "for i_step in range(n_steps):\n",
    "    gradient = (2/m).X_b.T.dot(X_b.dot(theta_i)-y)\n",
    "    theta_i = theta_i - a * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate\n",
    "\n",
    "The step size $\\alpha$ is also called the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Good learning rate\n",
    "\n",
    "<center><img src=\"Lecture05_Images/bgd_alpha_good.png\" alt=\"data-layout\" style=\"height: 400px;\"/></center>\n",
    "\n",
    "[Source: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate too small \n",
    "\n",
    "Convergence will be slow.\n",
    "\n",
    "<center><img src=\"Lecture05_Images/bgd_alpha_small.png\" alt=\"data-layout\" style=\"height: 400px;\"/></center>\n",
    "\n",
    "[Source: Geron]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate too large \n",
    "\n",
    "May not converge.\n",
    "\n",
    "<center><img src=\"Lecture05_Images/bgd_alpha_large.png\" alt=\"data-layout\" style=\"height: 400px;\"/></center>\n",
    "\n",
    "[Source: Geron]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look into how a local minima can be avoided (ask the Prof?)\n",
    "# Constraints and local minima..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evolution of fitted curve \n",
    "\n",
    "Consider the evolution of the curve corresponding to the best fit parameters for the first 10 iterations for different learning rates $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta_path_bgd = []\n",
    "\n",
    "def plot_gradient_descent(theta, alpha, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - alpha * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\alpha = {}$\".format(alpha), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\alpha=0.02$: learning rate is too small and will take a long time to reach the global optimum.\n",
    "- $\\alpha=0.1$: learning rate is good and global optimum will be found quickly.\n",
    "- $\\alpha=0.5$: learning rate is too large and parameters jump about (may not converge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function may not be nice with a single local minimum that coincides with the global minimum.\n",
    "\n",
    "<center><img src=\"Lecture05_Images/bgd_pitfalls.png\" alt=\"data-layout\" style=\"height: 300px;\"/></center>\n",
    "\n",
    "[Source: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convex objective function\n",
    "\n",
    "MSE cost function for linear regression is convex (for any two points on the curve, the line segment connecting those points lies above the curve).\n",
    "\n",
    "Consequently, the cost function has a single local minimum (that therefore coincides with the global minimum).\n",
    "\n",
    "Furthermore, the cost function is smooth with a slope that never changes abruptly (i.e. it is Lipschitz continuous).  \n",
    "\n",
    "Gradient descent is therefore guaranteed to converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradient descent to work well it is critical for all features to have a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Lecture05_Images/bgd_feature_scaling.png\" alt=\"data-layout\" style=\"height: 200px;\"/></center>\n",
    "\n",
    "[Source: Geron]\n",
    "\n",
    "In the case on the left, the features have the same scale and even intially the solution moves towards the minimum, thus reaching it quickly.\n",
    "\n",
    "In the case on the right, the features have different scales and initially the solution does not move directly towards the minimum.  Convergence is therefore slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into starting parameters for SGD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature scaling methods\n",
    "\n",
    "In general should make sure features have a similar scale since many machine learning methods do not work well when numerical attributes have very different scales.\n",
    "\n",
    "- Min-max scaling: \n",
    "$$x_j \\rightarrow \\frac{x_j - \\min_i x_i}{\\max_i x_i - \\min_i x_i}$$\n",
    "See Scikit-Learn [`MinMaxScalar`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
    "\n",
    "\n",
    "- Standardization: \n",
    "$$x_j \\rightarrow \\frac{x_j - \\mu_j}{\\sigma_j},$$\n",
    "where $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of feature $j$ computed from the training instances.\n",
    "See Scikit-Learn [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
